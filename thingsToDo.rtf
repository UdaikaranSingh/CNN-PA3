{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf100
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fnil\fcharset0 Georgia;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red82\green0\blue83;
}
{\*\expandedcolortbl;;\csgray\c0\c0;\cssrgb\c0\c0\c0;\cssrgb\c40000\c0\c40000;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww14200\viewh16580\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
Things to do:\
\
Part 1:\

\f1\b 1. Question 1\
2. Question 2\
3. Question 3
\f0\b0 \

\f1\b 4. Question 4
\f0\b0 \
\
Part 2:\

\f1\b 1. Finish baseline_cnn.py\
2. Implement train_model.ipynb\

\f0\b0 3. Part 1:\
	- implement 4 different scoring functions (in python notebook)\
	- implement way to create confusion matrix\
	- implement a summation of all 4 scores\
4. Part 2:\
	- Test Implementation\
5. Part 3:\
	(i)\
		- Test transformations on the Input images\
	(ii)\
		- Test 2 distinct architectures\
		- Save the best performing architecture\
	(iii)\
		- Address rare class problem\
			- Things to try: (1) Weighted Objective Function (2) add training data from under-represented classes (SMOTE 			module in Python)\
\
6. Test Everything (and record data):\
7. Write up Report 	\
\
Note:\
1. http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf\
	- used as the source for architecture 1, specifically adding sigmoid layer after max pooling layer.\
\
2. https://arxiv.org/pdf/1705.02315.pdf\
	- source of the weighted loss function\
\
3. Another paper to read for hyper-parameter tuning: https://arxiv.org/pdf/1606.02228.pdf\
\
4. Source for architecture 2 came from copying GoogleNet (lecture 11), basically the idea of increasing features deeper into the model)\
	The idea behind architecture 2 is that the increased convolutional layers will allow for better learning of the features, so to 	reduce the computation burden, the max-pooling was increased to be a 8x8 kernel.\
\
5. Over-sampling notes: https://beckernick.github.io/oversampling-modeling/ \
\
6. Link to how to visualize weights: https://discuss.pytorch.org/t/how-to-visualize-the-actual-convolution-filters-in-cnn/13850\
\
\
\
Note:\
1. Have to convert the logistic outputs to a n-hot array based on being > 0.5 (or some other threshold)\
\
\
Note:\
1. Should implement a loop that tests for best classification threshold for each class.\
\
Things to do:\
1. Confusion matrix\
2. Accuracy\
3. Weighted loss \
\
\cb2 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl280\sa120\partightenfactor0
\ls1\ilvl0
\f2 \cf3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec3 Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri: \'93ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases\'94, 2017;\'a0{\field{\*\fldinst{HYPERLINK "http://arxiv.org/abs/1705.02315"}}{\fldrslt \cf4 \strokec4 arXiv:1705.02315}}.\cb1 \
}