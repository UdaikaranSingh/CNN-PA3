{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Experiment 4: Architecture (1 or 2) with Normalized Inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding accuracy \n",
    "def accuracy_score(outputs, targets, batch_size):\n",
    "    out = output_to_guess(outputs, 0.5)\n",
    "    numCorrect = int(torch.sum(torch.sum((out == targets), dim=1)))\n",
    "    total = batch_size * 14\n",
    "    return numCorrect / total\n",
    "\n",
    "\n",
    "#helper function to convert a network output to a guess\n",
    "def output_to_guess(output, cutoff):\n",
    "    # create a temporary tensor to not override original\n",
    "    temp = output.clone()\n",
    "    temp[output >= 0.5] = 1\n",
    "    temp[output < 0.5] = 0\n",
    "    return temp\n",
    "\n",
    "# printing performce of model\n",
    "def printModelPerformance(performanceList):\n",
    "    accuracy_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    bcr_list = []\n",
    "    for i in range(len(performanceList)):\n",
    "        ac, pre, re, bcr = printSingleStat(performanceList, i)\n",
    "        accuracy_list.append(ac)\n",
    "        precision_list.append(pre)\n",
    "        recall_list.append(re)\n",
    "        bcr_list.append(bcr)\n",
    "    print(\"Model Average Performece\")\n",
    "    print(\"Accuracy: \", round((sum(accuracy_list) / len(accuracy_list)), 3))\n",
    "    print(\"Precision: \", round((sum(precision_list) / len(precision_list)), 3))\n",
    "    print(\"Recall: \", round((sum(recall_list) / len(recall_list)), 3))\n",
    "    print(\"BCR: \", round((sum(bcr_list) / len(bcr_list)),3))\n",
    "    \n",
    "def printSingleStat(performanceList, index):\n",
    "    disease = test_loader.dataset.classes[index]\n",
    "    TP = p_r_list[index][\"TP\"]\n",
    "    FP = p_r_list[index][\"FP\"]\n",
    "    TN = p_r_list[index][\"TN\"]\n",
    "    FN = p_r_list[index][\"FN\"]\n",
    "    \n",
    "    Acc = (TN + TP) / (TP + TN + FP + FN)\n",
    "    Precision = (TP) / (FP + TP)\n",
    "    Recall = (TP) / (TP + FN)\n",
    "    BCR = (Precision + Recall) / 2\n",
    "    print(\"Disease: \", disease)\n",
    "    print(\"Accuracy: \", round(Acc,3))\n",
    "    print(\"Precision: \", round(Precision,3))\n",
    "    print(\"Recall: \", round(Recall,3))\n",
    "    print(\"BCR: \", round(BCR,3))\n",
    "    \n",
    "    return Acc, Precision, Recall, BCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'mean' and 'std'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c9e5f4da4615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'mean' and 'std'"
     ]
    }
   ],
   "source": [
    "#TODO: change to architecture 1 if #1 was better\n",
    "from architecture2_cnn import *\n",
    "from architecture2_cnn import arch2_cnn\n",
    "from copy import deepcopy\n",
    "\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 1           # Number of full passes through the dataset\n",
    "batch_size = 32        # Number of samples in each minibatch\n",
    "learning_rate = 0.0001  \n",
    "seed = np.random.seed(1) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "train_loader, val_loader, test_loader = create_split_loaders(batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n",
    "\n",
    "model = arch2_cnn()\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "criterion = torch.nn.BCELoss(size_average = True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "all_valid_loss = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_model = model\n",
    "\n",
    "# Begin training procedure\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    N = 50\n",
    "    Validation_N = 250\n",
    "    N_minibatch_loss = 0.0\n",
    "    N_minibatch_accuracy = 0.0\n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "        \n",
    "\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        \n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = accuracy_score(outputs, labels, batch_size)\n",
    "        \n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        N_minibatch_accuracy += accuracy\n",
    "        \n",
    "            \n",
    "        if (minibatch_count % N == 0) & (minibatch_count > 0):    \n",
    "            \n",
    "            # Print the loss averaged over the last N mini-batches    \n",
    "            N_minibatch_loss /= N\n",
    "            N_minibatch_accuracy /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "            \n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            print(\"accuracy\", N_minibatch_accuracy)\n",
    "            train_accuracies.append(N_minibatch_accuracy)\n",
    "            N_minibatch_loss = 0.0\n",
    "            N_minibatch_accuracy = 0.0\n",
    "            \n",
    "            \n",
    "        if (minibatch_count % Validation_N == 0) & (minibatch_count > 0):    \n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad(): \n",
    "                for minibatch_count, (images, labels) in enumerate(val_loader, 0):\n",
    "                    # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "                    images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "                    \n",
    "                    outputs = model(images)\n",
    "                    val_loss += criterion(outputs, labels)\n",
    "                    accuracy += accuracy_score(outputs, labels, batch_size)\n",
    "                \n",
    "                avgLoss = val_loss / minibatch_count\n",
    "                valAccuracy = accuracy / minibatch_count\n",
    "                \n",
    "                print(\"Val Loss\", avgLoss.item())\n",
    "                print(\"Accuracy\", valAccuracy)\n",
    "                #checks if model is best model so far\n",
    "                if (len(all_valid_loss) == 0):\n",
    "                    model = deepcopy(model)\n",
    "                else:\n",
    "                    if (avgLoss.item() < min(all_valid_loss)):\n",
    "                        model = deepcopy(model)\n",
    "                all_valid_loss.append(avgLoss.item())\n",
    "                val_accuracies.append(valAccuracy)\n",
    "\n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "print(\"Training complete after\", epoch + 1, \"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert plots of training and validation stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert images of filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test based on best_model variable\n",
    "\n",
    "print(len(test_loader))\n",
    "\n",
    "p_r_list = []    #list of dictionaries, where each dictionary contains 'TP', 'FP', 'TN', 'FN'\n",
    "\n",
    "for i in range(14):\n",
    "    p_r_list.append({'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0})\n",
    "\n",
    "c_matrix = []     # confusion matrix\n",
    "\n",
    "for i in range(14):\n",
    "    c_matrix.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "for minibatch_count, (images, labels) in enumerate(test_loader, 0):\n",
    "    \n",
    "    print(minibatch_count)\n",
    "    # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "    images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "    \n",
    "    outputs = model(images)\n",
    "    outputs = output_to_guess(outputs, 0.5)\n",
    "    with torch.no_grad():\n",
    "        for i in range(outputs.shape[0]):\n",
    "            out = outputs.split(1)[i].tolist()[0]    # convert the outputs from tensor object to a list\n",
    "            lab = labels.split(1)[i].tolist()[0]     # convert the labels from tensor object to a list\n",
    "\n",
    "            for j in range(14):\n",
    "                if out[j] == 1 and lab[j] == 1:\n",
    "                    p_r_list[j]['TP'] += 1\n",
    "                elif out[j] == 1 and lab[j] == 0:\n",
    "                    p_r_list[j]['FP'] += 1\n",
    "                elif out[j] == 0 and lab[j] == 0:\n",
    "                    p_r_list[j]['TN'] += 1\n",
    "                else:\n",
    "                    p_r_list[j]['FN'] += 1\n",
    "\n",
    "            for k in range(14):\n",
    "                if (out[k] == 1 and lab[k] == 1) or (out[k] == 1 and lab[k] == 1):     # if output == label, put a 1 at (k,k) and zeros for everything else in that row\n",
    "                    c_matrix[k][k] += 1\n",
    "                if out[k] == 1 and lab[k] == 0:   # if output = 1 and label = 0, evenly distribute the error amongst activated outputs (where out[k] == 1)\n",
    "                    sum_of_out = sum(out)\n",
    "                    for a in range(14):\n",
    "                        if out[a] == 1:\n",
    "                            c_matrix[k][a] += 1/sum_of_out\n",
    "                if out[k] == 0 and lab[k] == 1:   # if output = 0 and label = 1, put zeros for the row (it's not confused with other classes)\n",
    "                    continue\n",
    "\n",
    "for x in range(len(c_matrix)):\n",
    "    row_sum = sum(c_matrix[x])\n",
    "    for y in range(14):\n",
    "        c_matrix[x][y] = c_matrix[x][y] / row_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printModelPerformance(p_r_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print confusion matrix\n",
    "\n",
    "for i in c_matrix:\n",
    "    a = ['%.2f' % elem for elem in i]\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
