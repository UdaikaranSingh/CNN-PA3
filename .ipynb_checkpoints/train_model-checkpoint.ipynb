{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f634df113d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCustom_BCE_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustom_BCE_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "class Custom_BCE_Loss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Custom_BCE_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        #puts a range on the outputs to avoid a nan error\n",
    "        output = output.clamp(min=1e-7, max=1-1e-7)\n",
    "        target = target.float()\n",
    "        \n",
    "        w_n = 1\n",
    "        w_p = 2\n",
    "        \n",
    "        loss = (-1) * w_n * (target * torch.log(output)) - w_p * ((1 - target) * torch.log(1 - output))\n",
    "    \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_cnn import *\n",
    "from baseline_cnn import BasicCNN\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 1           # Number of full passes through the dataset\n",
    "batch_size = 32        # Number of samples in each minibatch\n",
    "learning_rate = 0.00001  \n",
    "seed = np.random.seed(1) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "#transform = transforms.Compose(______) #starter codse\n",
    "transform = transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "train_loader, val_loader, test_loader = create_split_loaders(batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n",
    "\n",
    "# Instantiate a BasicCNN to run on the GPU or CPU based on CUDA support\n",
    "model = BasicCNN()\n",
    "model = model.to(computing_device)\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "#TODO: Define the loss criterion and instantiate the gradient descent optimizer\n",
    "#criterion = ______ #TODO - loss criteria are defined in the torch.nn package\n",
    "criterion = torch.nn.BCELoss(size_average = True)\n",
    "#TODO: change loss function to weighted loss\n",
    "\n",
    "#TODO: Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "#optimizer = ______ #TODO - optimizers are defined in the torch.optim package #this is the start code line\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(outputs, targets, batch_size):\n",
    "    out = output_to_guess(outputs, 0.5)\n",
    "    numCorrect = int(torch.sum(torch.sum((out == targets), dim=1)))\n",
    "    total = batch_size * 14\n",
    "    return numCorrect / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to convert a network output to a guess\n",
    "def output_to_guess(output, cutoff):\n",
    "    # we create a copy of the original tensor, \n",
    "    # because of the way we are replacing them.\n",
    "    temp = output.clone()\n",
    "    temp[output >= 0.5] = 1\n",
    "    temp[output < 0.5] = 0\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "all_valid_loss = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_model = model\n",
    "\n",
    "# Begin training procedure\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    N = 50\n",
    "    Validation_N = 100\n",
    "    N_minibatch_loss = 0.0\n",
    "    N_minibatch_accuracy = 0.0\n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "        \n",
    "\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        \n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        accuracy = accuracy_score(outputs, labels, batch_size)\n",
    "        \n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        N_minibatch_accuracy += accuracy\n",
    "        \n",
    "            \n",
    "        if (minibatch_count % N == 0) & (minibatch_count > 0):    \n",
    "            \n",
    "            # Print the loss averaged over the last N mini-batches    \n",
    "            N_minibatch_loss /= N\n",
    "            N_minibatch_accuracy /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f' %\n",
    "                (epoch + 1, minibatch_count, N_minibatch_loss))\n",
    "            \n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            print(\"accuracy\", N_minibatch_accuracy)\n",
    "            train_accuracies.append(N_minibatch_accuracy)\n",
    "            N_minibatch_loss = 0.0\n",
    "            N_minibatch_accuracy = 0.0\n",
    "            \n",
    "            \n",
    "        if (minibatch_count % Validation_N == 0) & (minibatch_count > 0):    \n",
    "            val_loss = 0\n",
    "            accuracy = 0\n",
    "            with torch.no_grad(): \n",
    "                for minibatch_count, (images, labels) in enumerate(val_loader, 0):\n",
    "                    # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "                    images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "                    \n",
    "                    outputs = model(images)\n",
    "                    val_loss += criterion(outputs, labels)\n",
    "                    accuracy += accuracy_score(outputs, labels, batch_size)\n",
    "                \n",
    "                avgLoss = val_loss / minibatch_count\n",
    "                valAccuracy = accuracy / minibatch_count\n",
    "                \n",
    "                print(\"Val Loss\", avgLoss.item())\n",
    "                print(\"Accuracy\", valAccuracy)\n",
    "                #checks if model is best model so far\n",
    "                if (len(all_valid_loss) == 0):\n",
    "                    model = deepcopy(model)\n",
    "                else:\n",
    "                    if (avgLoss.item() < min(all_valid_loss)):\n",
    "                        model = deepcopy(model)\n",
    "                all_valid_loss.append(avgLoss.item())\n",
    "                val_accuracies.append(valAccuracy)\n",
    "\n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "print(\"Training complete after\", epoch + 1, \"epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-39aefb74c1c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#test based on best_model variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mminibatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mp_r_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m#list of dictionaries, where each dictionary contains 'TP', 'FP', 'TN', 'FN'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "#Testing Block (looking to find all the statistics and confusion matrix)\n",
    "#test based on best_model variable\n",
    "\n",
    "minibatch_count = len(test_loader)\n",
    "print(minibatch_count)\n",
    "\n",
    "p_r_list = []    #list of dictionaries, where each dictionary contains 'TP', 'FP', 'TN', 'FN'\n",
    "\n",
    "for i in range(14):\n",
    "    p_r_list.append({'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0})\n",
    "\n",
    "c_matrix = []     # confusion matrix\n",
    "\n",
    "for i in range(14):\n",
    "    c_matrix.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "for minibatch_count, (images, labels) in enumerate(test_loader, 0):\n",
    "    \n",
    "    print(minibatch_count)\n",
    "    # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "    images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "    \n",
    "    outputs = model(images)\n",
    "    outputs = output_to_guess(outputs, 0.5)\n",
    "    with torch.no_grad():\n",
    "        for i in range(outputs.shape[0]):\n",
    "            out = outputs.split(1)[i].tolist()[0]    # convert the outputs from tensor object to a list\n",
    "            lab = labels.split(1)[i].tolist()[0]     # convert the labels from tensor object to a list\n",
    "\n",
    "            for j in range(14):\n",
    "                if out[j] == 1 and lab[j] == 1:\n",
    "                    p_r_list[j]['TP'] += 1\n",
    "                elif out[j] == 1 and lab[j] == 0:\n",
    "                    p_r_list[j]['FP'] += 1\n",
    "                elif out[j] == 0 and lab[j] == 0:\n",
    "                    p_r_list[j]['TN'] += 1\n",
    "                else:\n",
    "                    p_r_list[j]['FN'] += 1\n",
    "\n",
    "            for k in range(14):\n",
    "                if (out[k] == 1 and lab[k] == 1) or (out[k] == 1 and lab[k] == 1):     # if output == label, put a 1 at (k,k) and zeros for everything else in that row\n",
    "                    c_matrix[k][k] += 1\n",
    "                if out[k] == 1 and lab[k] == 0:   # if output = 1 and label = 0, evenly distribute the error amongst activated outputs (where out[k] == 1)\n",
    "                    sum_of_out = sum(out)\n",
    "                    for a in range(14):\n",
    "                        if out[a] == 1:\n",
    "                            c_matrix[k][a] += 1/sum_of_out\n",
    "                if out[k] == 0 and lab[k] == 1:   # if output = 0 and label = 1, put zeros for the row (it's not confused with other classes)\n",
    "                    continue\n",
    "\n",
    "for x in range(len(c_matrix)):\n",
    "    row_sum = sum(c_matrix[x])\n",
    "    for y in range(14):\n",
    "        c_matrix[x][y] = c_matrix[x][y] / row_sum\n",
    "\n",
    "\n",
    "    print(c_matrix)\n",
    "    \n",
    "#     for i in range(len(outputs_matrix)):\n",
    "#         for j in range(14):\n",
    "#             if outputs_matrix[i][j] == 1 and labels_matrix[i][j] == 1: \n",
    "#                 p_r_list[j]['TP'] += 1\n",
    "#             elif outputs_matrix[i][j] == 1 and labels_matrix[i][j] == 0:\n",
    "#                 p_r_list[j]['FP'] += 1\n",
    "#             elif outputs_matrix[i][j] == 0 and labels_matrix[i][j] == 0:\n",
    "#                 p_r_list[j]['TN'] += 1\n",
    "#             else:\n",
    "#                 p_r_list[j]['FN'] += 1\n",
    "   \n",
    "    \n",
    "    #complete tests (all 4 scores) (for each class)\n",
    "    \"\"\"\n",
    "    Idea:\n",
    "    1. Iterate through the output vs. label\n",
    "    2. Place each correct/incorrect into piles of: true negatives, false negatives, true positives, and false positives\n",
    "        - note: different pile for each class\n",
    "        - note: could make a matrix of size (labels + 1 x 4) where the i input is the true label and the j input is the correct category\n",
    "    3. Use these piles to calculate score for each class\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #complete confusion matrix\n",
    "    \"\"\"\n",
    "    Idea:\n",
    "    1. Create matrix of size (labels x labels)\n",
    "    2. Iterate through output vs. labels\n",
    "    3. increase value by 1 in each corresponding i,j spot\n",
    "    4. Divide by average across each row afterwards.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Loss Function\n",
    "#Based on a weights on Negatives and Positives\n",
    "def weighted_Loss_Function(output, target, w_n = None, w_p = None):\n",
    "    \n",
    "    #puts a range on the outputs to avoid a nan error\n",
    "    output = output.clamp(min=1e-7, max=1-1e-7)\n",
    "    \n",
    "    target = target.float()\n",
    "    if (w_n == None & w_p == None):\n",
    "        #default loss function if no weight parameters given\n",
    "        loss = (-1) * target * torch.log(output) - (1 - target) * torch.log(1 - output)\n",
    "    else:\n",
    "        loss = (-1) * w_n * (target * torch.log(output)) - w_p * ((1 - target) * torch.log(1 - output))\n",
    "    \n",
    "    return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to find the w_n and w_p parameters. \n",
    "\"\"\"\n",
    "w_n = (|n| + |p|) / |n| \n",
    "\n",
    "w_p = (|n| + |p|) / |p| \n",
    "\n",
    "Goal: find the number of negative and positive outputs in the outputs.\n",
    "Positives = they have a disease\n",
    "Negative = they do not have a disease\n",
    "\n",
    "Procedure: loop through the outputs hot-coded arrays. If there is a 1, then it counts as positive\n",
    "\"\"\"\n",
    "\n",
    "numNeg = 0\n",
    "numPos = 0\n",
    "minibatch_count = test_loader.dataset.labels.size / batch_size\n",
    "for minibatch_count, (images, labels) in enumerate(test_loader, 0):\n",
    "    for row in labels.split(1):\n",
    "        x = row.numpy().sum()\n",
    "        numPos = numPos + x\n",
    "        numNeg = numNeg + (14 - x)\n",
    "\n",
    "w_n = (numNeg + numPos) / numNeg\n",
    "w_p = (numNeg + numPos) / numPos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
